import albumentations as A
import argparse
import cv2
import numpy as np
import os
import timm
import torch
import torch.nn as nn
import torch.optim as optim
import torch.backends.cudnn as cudnn

# ğŸ”¥ å¼•å…¥æŒ‡æ ‡è®¡ç®—
from sklearn.metrics import classification_report

from PIL import Image
from albumentations.pytorch import ToTensorV2
from pathlib import Path
from torch.optim import lr_scheduler
from torch.utils.tensorboard import SummaryWriter
from torchvision.datasets import ImageFolder
from tqdm import tqdm
from typing import Tuple

# è®¾ç½®è®¾å¤‡ (ä¼˜å…ˆä½¿ç”¨ GPU)
cudnn.benchmark = True
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")


def parse_args():
    # åˆ›å»ºå‚æ•°è§£æå™¨
    parser = argparse.ArgumentParser(description="å­—ä½“åˆ†ç±»è®­ç»ƒè„šæœ¬")

    # æ·»åŠ å‚æ•°
    parser.add_argument(
        "--image_folder",
        type=str,
        default="/root/autodl-tmp/font-classify/dataset_fonts",
        help="åŒ…å«å›¾ç‰‡æ•°æ®çš„æ–‡ä»¶å¤¹è·¯å¾„",
    )
    parser.add_argument(
        "--output_folder",
        type=str,
        default="/root/autodl-tmp/font-classify/model_all", # ğŸ”¥ æ”¹äº†åå­—ï¼Œé¿å…è¦†ç›–æ—§æ¨¡å‹
        help="è®­ç»ƒå¥½çš„æ¨¡å‹ä¿å­˜è·¯å¾„",
    )
    parser.add_argument(
        "--test_split",
        type=float,
        default=0.15,
        help="ç”¨äºæµ‹è¯•ï¼ˆéªŒè¯ï¼‰çš„æ•°æ®é›†æ¯”ä¾‹ (ä¾‹å¦‚ 0.15 è¡¨ç¤º 15%)",
    )
    parser.add_argument(
        "-net",
        "--network_type",
        type=str,
        default="resnet50",
        help="ä½¿ç”¨çš„ç½‘ç»œæ¶æ„ç±»å‹ (ä¾‹å¦‚ resnet50)",
    )
    parser.add_argument("-bs", "--batch_size", type=int, default=32, help="æ‰¹å¤„ç†å¤§å° (Batch size)")
    parser.add_argument(
        "-lr", "--learning_rate", type=float, default=0.0001, help="åˆå§‹å­¦ä¹ ç‡"
    )
    parser.add_argument(
        "-e", "--num_epochs", type=int, default=100, help="è®­ç»ƒæ€»è½®æ•° (Epochs)"
    )
    parser.add_argument(
        "--num_workers", type=int, default=4, help="æ•°æ®åŠ è½½å™¨çš„å·¥ä½œçº¿ç¨‹æ•°"
    )

    # è§£æå‚æ•°
    args = parser.parse_args()

    return args


class CustomImageFolder(ImageFolder):
    """è‡ªå®šä¹‰å›¾åƒæ–‡ä»¶å¤¹åŠ è½½å™¨ï¼Œæ”¯æŒ Albumentations å¢å¼º"""
    def __init__(self, root, transform=None, **kwargs):
        super(CustomImageFolder, self).__init__(root, **kwargs)
        self.transform = transform

    def __getitem__(self, index):
        path, target = self.samples[index]
        sample = Image.open(path).convert("RGB") # ç¡®ä¿è½¬ä¸º RGB é˜²æ­¢å•é€šé“æŠ¥é”™

        if self.transform is not None:
            sample = np.array(sample)  # å°† PIL å›¾ç‰‡è½¬ä¸º Numpy æ•°ç»„
            transformed = self.transform(image=sample)  # åº”ç”¨å¢å¼º
            sample = transformed["image"]  # æå–å¢å¼ºåçš„å›¾ç‰‡

        return sample, target


class ResizeWithPad:
    """ä¿æŒçºµæ¨ªæ¯”ç¼©æ”¾å¹¶å¡«å……èƒŒæ™¯"""
    def __init__(
        self, new_shape: Tuple[int, int], padding_color: Tuple[int] = (255, 255, 255)
    ) -> None:
        self.new_shape = new_shape
        self.padding_color = padding_color

    def __call__(self, image: np.array, **kwargs) -> np.array:
        original_shape = (image.shape[1], image.shape[0])
        ratio = float(max(self.new_shape)) / max(original_shape)
        new_size = tuple([int(x * ratio) for x in original_shape])
        image = cv2.resize(image, new_size)
        delta_w = self.new_shape[0] - new_size[0]
        delta_h = self.new_shape[1] - new_size[1]
        top, bottom = delta_h // 2, delta_h - (delta_h // 2)
        left, right = delta_w // 2, delta_w - (delta_w // 2)
        image = cv2.copyMakeBorder(
            image,
            top,
            bottom,
            left,
            right,
            cv2.BORDER_CONSTANT,
            value=self.padding_color,
        )
        return image


class CutMax:
    """å¦‚æœå›¾ç‰‡è¶…è¿‡æœ€å¤§å°ºå¯¸ï¼Œåˆ™è¿›è¡Œè£å‰ª"""

    def __init__(self, max_size: int = 1024) -> None:
        self.max_size = max_size

    def __call__(self, image: np.array, **kwargs) -> np.array:
        if image.shape[0] > self.max_size:
            image = image[: self.max_size, :, :]
        if image.shape[1] > self.max_size:
            image = image[:, : self.max_size, :]
        return image


def main(args):
    os.makedirs(args.output_folder, exist_ok=True)

    # ============================================================
    # ğŸ› ï¸ å…³é”®ä¿®æ”¹ï¼šä¼˜åŒ–åçš„æ•°æ®å¢å¼ºç­–ç•¥
    # ============================================================
    print("ğŸ”§ æ­£åœ¨åº”ç”¨ä¼˜åŒ–åçš„æ•°æ®å¢å¼ºç­–ç•¥ (å·²ä¿®å¤éš¶ä¹¦/å¹¼åœ†è¯†åˆ«é—®é¢˜)...")
    transform = A.Compose(
        [
            A.Lambda(image=CutMax(1024)),
            A.Lambda(image=ResizeWithPad((320, 320))),  # è‡ªå®šä¹‰æ–¹å½¢å¡«å……
            
            # ğŸ”¥ ä¿®æ”¹ 1ï¼šé’ˆå¯¹éš¶ä¹¦ï¼Œå¤§å¹…é™ä½æ—‹è½¬è§’åº¦ (60 -> 10)
            A.ShiftScaleRotate(
                shift_limit=0.1,        # ç¨å¾®å¹³ç§»
                scale_limit=(0.9, 1.1), # ç¼©æ”¾å¹…åº¦å‡å°
                rotate_limit=10,        # å…³é”®ï¼ä¿æŠ¤éš¶ä¹¦ç»“æ„ä¸è¢«ç ´å
                interpolation=1,
                p=0.5,
            ),
            
            # é¢œè‰²å¢å¼ºä¿ç•™ï¼Œå¢åŠ æ¨¡å‹å¯¹å…‰ç…§çš„é²æ£’æ€§
            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05, p=0.3),
            
            # ğŸ”¥ ä¿®æ”¹ 2ï¼šç§»é™¤äº† ISONoise å’Œ ImageCompression
            # åˆ é™¤äº†è¿™ä¸¤è¡Œï¼Œä¿æŠ¤å¹¼åœ†çš„æ¸…æ™°åº¦ï¼Œé˜²æ­¢åœ†è§’å˜ç³Šè¢«è¯¯åˆ¤ä¸ºé»‘ä½“
            # A.ISONoise(p=0.2), 
            # A.ImageCompression(quality_lower=70, quality_upper=95, p=0.2),
            
            A.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            ToTensorV2(),
        ]
    )

    # é¢„è§ˆç”¨çš„ Transform (æ— äººçœ¼ä¸å¯è§çš„å½’ä¸€åŒ–)
    check_transform = A.Compose(
        [
            A.Lambda(image=CutMax(1024)),
            A.Lambda(image=ResizeWithPad((320, 320))),
            
            # ğŸ”¥ è¿™é‡Œå‚æ•°ä¸è®­ç»ƒä¿æŒä¸€è‡´ (10åº¦)ï¼Œç¡®ä¿é¢„è§ˆå›¾çœŸå®åæ˜ è®­ç»ƒæƒ…å†µ
            A.ShiftScaleRotate(
                shift_limit=0.1, 
                scale_limit=(0.9, 1.1), 
                rotate_limit=10, 
                interpolation=1, 
                p=0.5
            ),
            
            # é¢œè‰²å¢å¼ºé¢„è§ˆ
            A.ColorJitter(brightness=0.2, contrast=0.2, saturation=0.2, hue=0.05, p=0.3),
        ]
    )

    image_folder = args.image_folder
    network_type = args.network_type
    best_model_params_path = os.path.join(args.output_folder, "best_model_params.pt")

    # æ•°æ®é›†è®¾ç½®
    dataset = CustomImageFolder(image_folder, transform=transform)
    
    # è‡ªåŠ¨è¯†åˆ«å‰©ä¸‹çš„ç±»åˆ«
    class_names = dataset.classes
    print(f"âœ… æˆåŠŸæ£€æµ‹åˆ° {len(class_names)} ä¸ªåˆ†ç±»")
    
    n = len(dataset)
    n_test = int(args.test_split * n)
    train_dataset, test_dataset = torch.utils.data.random_split(
        dataset, [n - n_test, n_test]
    )

    # ä¿å­˜å¢å¼ºåçš„æ£€æŸ¥å›¾ç‰‡ (Check images)
    check_dataset = CustomImageFolder(image_folder, transform=check_transform)
    Path(os.path.join(args.output_folder, "check")).mkdir(parents=True, exist_ok=True)
    print("ğŸ’¾ æ­£åœ¨ä¿å­˜å¢å¼ºæ•ˆæœé¢„è§ˆå›¾ (Check images)...")
    for i in range(min(20, len(check_dataset))): # é™åˆ¶ä¿å­˜ 20 å¼ ä»¥èŠ‚çœæ—¶é—´
        img_data = check_dataset[i]
        img = img_data[0] # è·å–å›¾ç‰‡éƒ¨åˆ†
        Image.fromarray(img).save(os.path.join(args.output_folder, "check", f"{i}.png"))

    # ä¿å­˜ç±»åˆ«åç§°åˆ—è¡¨
    with open(os.path.join(args.output_folder, "class_names.txt"), "w") as f:
        for item in class_names:
            f.write(f"{item}\n")

    dataset_sizes = {"train": len(train_dataset), "val": len(test_dataset)}

    # æ•°æ®åŠ è½½å™¨ (Dataloaders)
    batch_size = args.batch_size
    train_dataloader = torch.utils.data.DataLoader(
        train_dataset, num_workers=args.num_workers, batch_size=batch_size, shuffle=True
    )
    test_dataloader = torch.utils.data.DataLoader(
        test_dataset, num_workers=args.num_workers, batch_size=batch_size, shuffle=True
    )
    dataloaders = {"train": train_dataloader, "val": test_dataloader}

    # æ¨¡å‹åˆ›å»º
    print(f"ğŸ—ï¸ æ­£åœ¨åˆ›å»ºæ¨¡å‹æ¶æ„: {network_type}")
    model = timm.create_model(
        network_type, pretrained=True, num_classes=len(class_names)
    )
    model.to(device)

    # æŸå¤±å‡½æ•° & ä¼˜åŒ–å™¨
    # ğŸ”¥ ä¿®æ”¹ 3ï¼šåŠ å…¥ label_smoothing (æ ‡ç­¾å¹³æ»‘) é˜²æ­¢æ¨¡å‹è¿‡äºè‡ªä¿¡ï¼Œç¼“è§£ç›¸ä¼¼å­—ä½“æ··æ·†
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    
    optimizer = optim.AdamW(
        model.parameters(), lr=args.learning_rate, weight_decay=1e-4
    )
    scheduler = lr_scheduler.CosineAnnealingWarmRestarts(
        optimizer, T_0=args.num_epochs, T_mult=1, eta_min=0
    )

    writer = SummaryWriter(log_dir=os.path.join(args.output_folder, "runs"))

    # è®­ç»ƒå¾ªç¯
    best_acc = 0.0

    for epoch in range(args.num_epochs):
        print(f"\nè½®æ¬¡ (Epoch) {epoch}/{args.num_epochs - 1}")
        print("-" * 10)

        for phase in ["train", "val"]:
            if phase == "train":
                model.train()
            else:
                model.eval()

            running_loss = 0.0
            running_corrects = 0
            
            # ç”¨äºè®¡ç®—è¯¦ç»†æŒ‡æ ‡çš„å®¹å™¨
            val_preds = []
            val_labels = []

            for inputs, labels in tqdm(dataloaders[phase], desc=f"{phase} é˜¶æ®µ"):
                inputs = inputs.to(device)
                labels = labels.to(device)

                optimizer.zero_grad()

                with torch.set_grad_enabled(phase == "train"):
                    with torch.cuda.amp.autocast():
                        outputs = model(inputs)
                        _, preds = torch.max(outputs, 1)
                        loss = criterion(outputs, labels)

                    if phase == "train":
                        loss.backward()
                        optimizer.step()

                running_loss += loss.item() * inputs.size(0)
                running_corrects += torch.sum(preds == labels.data)

                # å¦‚æœæ˜¯éªŒè¯é˜¶æ®µï¼Œæ”¶é›†æ•°æ®ç”¨äºç”Ÿæˆåˆ†ç±»æŠ¥å‘Š
                if phase == "val":
                    val_preds.extend(preds.cpu().numpy())
                    val_labels.extend(labels.cpu().numpy())

            if phase == "train":
                scheduler.step()

            epoch_loss = running_loss / dataset_sizes[phase]
            epoch_acc = running_corrects.double() / dataset_sizes[phase]

            print(f"{phase} æŸå¤±(Loss): {epoch_loss:.4f} å‡†ç¡®ç‡(Acc): {epoch_acc:.4f}")

            # TensorBoard æ—¥å¿—è®°å½•
            writer.add_scalar(f"{phase}/Loss", epoch_loss, epoch)
            writer.add_scalar(f"{phase}/Accuracy", epoch_acc, epoch)

            # è¯¦ç»†çš„æ¯ç±»æŒ‡æ ‡è®°å½• (ä»…åœ¨éªŒè¯é˜¶æ®µ Val)
            if phase == "val":
                print("\nğŸ“Š åˆ†ç±»è¯¦æƒ…æŠ¥å‘Š (Classification Report):")
                # 1. æ‰“å°äººç±»å¯è¯»çš„è¡¨æ ¼æŠ¥å‘Š
                print(classification_report(val_labels, val_preds, target_names=class_names, digits=4))
                
                # 2. è·å–å­—å…¸æ ¼å¼ä»¥ä¾¿å†™å…¥ TensorBoard
                report_dict = classification_report(val_labels, val_preds, target_names=class_names, output_dict=True)
                
                # 3. å†™å…¥ TensorBoard
                for cls_name in class_names:
                    if cls_name in report_dict:
                        writer.add_scalar(f"Class_F1/{cls_name}", report_dict[cls_name]['f1-score'], epoch)
                        writer.add_scalar(f"Class_Precision/{cls_name}", report_dict[cls_name]['precision'], epoch)
                        writer.add_scalar(f"Class_Recall/{cls_name}", report_dict[cls_name]['recall'], epoch)
                
                writer.add_scalar("Overall/Macro_F1", report_dict['macro avg']['f1-score'], epoch)

            if phase == "val" and epoch_acc > best_acc:
                best_acc = epoch_acc
                torch.save(model.state_dict(), best_model_params_path)

        print(f"å½“å‰æœ€ä½³éªŒè¯å‡†ç¡®ç‡: {best_acc:.4f}")

    # åŠ è½½æœ€ä½³æ¨¡å‹å¹¶ä¿å­˜æœ€ç»ˆç‰ˆæœ¬
    model.load_state_dict(torch.load(best_model_params_path))
    torch.save(
        model.state_dict(), os.path.join(args.output_folder, "trained_model.pth")
    )

    writer.close()
    print("ğŸš€ è®­ç»ƒå…¨éƒ¨å®Œæˆï¼")


if __name__ == "__main__":
    args = parse_args()
    main(args)